<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
        <title>Python - Tag - 我的全新 Hugo 网站</title>
        <link>http://example.org/tags/python/</link>
        <description>Python - Tag - 我的全新 Hugo 网站</description>
        <generator>Hugo -- gohugo.io</generator><language>zh-CN</language><lastBuildDate>Tue, 18 Apr 2023 14:18:50 &#43;0000</lastBuildDate><atom:link href="http://example.org/tags/python/" rel="self" type="application/rss+xml" /><item>
    <title>Pandas 一列分多列，一行分多行，提取列中的要素，实现vlookup功能等</title>
    <link>http://example.org/pandas%E4%B8%80%E5%88%97%E5%88%86%E5%A4%9A%E5%88%97%E4%B8%80%E8%A1%8C%E5%88%86%E5%A4%9A%E8%A1%8C%E7%9A%84%E5%AE%9E%E7%8E%B0/</link>
    <pubDate>Tue, 18 Apr 2023 14:18:50 &#43;0000</pubDate>
    <author>五彩斑斓的黑</author>
    <guid>http://example.org/pandas%E4%B8%80%E5%88%97%E5%88%86%E5%A4%9A%E5%88%97%E4%B8%80%E8%A1%8C%E5%88%86%E5%A4%9A%E8%A1%8C%E7%9A%84%E5%AE%9E%E7%8E%B0/</guid>
    <description><![CDATA[转载自https://blog.csdn.net/Asher117/article/details/84346073
摘要 在进行数据分析时，我们经常需要把DataFrame的一列拆成多列或者根据某列把一行拆成多行，这篇文章主要讲解这两个目标的实现。 读取数据 一列分多列 将City列转成多列（以|为分隔符）,这里使用匿名函数lambda来将City列拆成两列。 一行分多行 将DataFrame一行拆成多行（以|为分隔符）
方法一 在刚刚得到的DataFrame基础上操作,如下图所以，可以明显看到我们按照City列将DataFrame拆成了多行。主要是先将DataFrame拆成多列，然后拆成多个DataFrame再使用concat组合。但是这种方法碰到City列切割不均匀的时候可能会麻烦一点，因此，这个时候你可以使用万能方法二。 方法二 这个方法的主要思想是，首先将DataFrame中需要拆分的列进行拆分，再使用stack（）进行轴变换，然后通过index来join即可，如下所示。 首先，将刚刚的df还原成原始形式： 接下来取出其City列，并切分成多列之后轴转换，之后重新设置索引，并且重命名为Company 最后删除df里面的Country列，并将DataFrame-df1 使用join到df里面得到最后的结果。 正则提取某列的要素并按要素分列 有个csv表格（以逗号,作为分隔符），数据样本如下，其中电话号码列中电话号码的个数不确定：
姓名 性别 电话号码 张三 男 电话号码:13788881111;+86-18911112222;其码:86-17722221111;其号码:+86 13565459999;86 15366558877 王五 女 16535468764;+77-16888779512;+86 13565457898;其他+87-13544569871 使用pandas库结合正则表达式将第三列中的手机号码提取后每个号码单独成列，与原来的数据一起形成新的行。 预期结果：
姓名 性别 电话号码 (0, 0) (0, 1) (0, 2) (0, 3) (0, 4) 张三 男 电话号码:13788881111;+86-18911112222;其码:86-17722221111;其号码:+86 13565459999;86 15366558877 13788881111 18911112222 17722221111 13565459999 15366558877 王五 女 16535468764;+77-16888779512;+86 13565457898;其他+87-13544569871 16535468764 16888779512 13565457898 13544569871 实现代码一 1 2 3 4 5 6 7 8 9 10 11 12 13 14 import pandas as pd df = pd.]]></description>
</item>
<item>
    <title>计算多边形镂空区域的坐标</title>
    <link>http://example.org/%E8%AE%A1%E7%AE%97%E5%A4%9A%E8%BE%B9%E5%BD%A2%E9%95%82%E7%A9%BA%E5%8C%BA%E5%9F%9F%E7%9A%84%E5%9D%90%E6%A0%87/</link>
    <pubDate>Sat, 11 Mar 2023 22:58:24 &#43;0000</pubDate>
    <author>五彩斑斓的黑</author>
    <guid>http://example.org/%E8%AE%A1%E7%AE%97%E5%A4%9A%E8%BE%B9%E5%BD%A2%E9%95%82%E7%A9%BA%E5%8C%BA%E5%9F%9F%E7%9A%84%E5%9D%90%E6%A0%87/</guid>
    <description><![CDATA[背景 已知多边形A（{&quot;Type&quot;:&quot;MultiPolygon&quot;}）以及其包含的多个子多边形（均为MultiPolygon）的GPS坐标，求多边形A包含的剩余1个子多边形的GPS坐标。
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 import geopandas as gpd from shapely.ops import unary_union, polygonize import pandas as pd # 读取geojson文件并转换为GeoDataFrame类型的数据 data = gpd.read_file(&#39;长沙市（缺一）.json&#39;) # 将所有多边形合并成一个单独的多边形 polygons = unary_union(data.geometry) # 将合并后的多边形转换为镂空多边形F,polygonize函数为关键 holes = list(polygonize(polygons)) # 将镂空多边形F转换为GeoDataFrame类型的数据 gdf_F = gpd.GeoDataFrame(geometry=holes, crs=data.crs) gdf_F[&#39;type&#39;] = &#39;F&#39; # 将A、B、C、D、E转换为GeoDataFrame类型的数据 gdf_ABCDE = data.copy() gdf_ABCDE[&#39;type&#39;] = gdf_ABCDE.]]></description>
</item>
<item>
    <title>Python和Golang超大文本文件处理</title>
    <link>http://example.org/python%E5%92%8Cgolang%E8%B6%85%E5%A4%A7%E6%96%87%E6%9C%AC%E6%96%87%E4%BB%B6%E5%A4%84%E7%90%86/</link>
    <pubDate>Tue, 20 Dec 2022 15:26:31 &#43;0000</pubDate>
    <author>五彩斑斓的黑</author>
    <guid>http://example.org/python%E5%92%8Cgolang%E8%B6%85%E5%A4%A7%E6%96%87%E6%9C%AC%E6%96%87%E4%BB%B6%E5%A4%84%E7%90%86/</guid>
    <description><![CDATA[Python Python中迭代读取超大文本文件，每次读取1000行。 在python中，我们可以使用yield关键字和for循环实现迭代读取超大文本文件，每次读取1000行。
例如，我们可以定义一个函数read_lines_by_1000()，每次yield 1000行的数据：
1 2 3 4 5 6 7 8 9 def read_lines_by_1000(filename): with open(filename, &#39;r&#39;) as f: lines = [] for line in f: lines.append(line) if len(lines) == 1000: yield lines lines = [] yield lines 然后，我们可以使用for循环迭代读取超大文本文件：
1 2 3 4 filename = &#39;data.txt&#39; for lines in read_lines_by_1000(filename): print(lines) # 处理lines中的数据 有比这个效率更高的方法吗 这样就可以在python中迭代读取超大文本文件，每次读取1000行了。 使用yield关键字和for循环迭代读取超大文本文件，每次读取1000行的方法已经是目前效率最高的方法之一。
如果想要提升效率，还可以考虑以下几种方法：
尽量使用内存映射文件读取超大文本文件，这样可以避免将数据从磁盘读取到内存中，从而提升读取速度。
尽量使用多线程读取超大文本文件，从而提升读取速度。
尽量使用多进程读取超大文本文件，从而提升读取速度。
尽量使用单个较大的缓冲区读取超大文本文件，从而提升读取速度。
需要注意的是，使用上述方法时可能会带来一些问题，例如内存占用过大，多线程/多进程同步问题等。因此，需要根据实际情况选择适合的方法。
python如何正确处理超大文本文件，防止内存吃不消。 在python中，可以使用以下几种方法正确处理超大文本文件，防止内存吃不消：
使用yield关键字和for循环迭代读取超大文本文件，每次读取一定数量的行。这样可以避免将整个文件一次性读取到内存中，从而防止内存吃不消。
使用内存映射文件读取超大文本文件。这样可以避免将数据从磁盘读取到内存中，从而防止内存吃不消。
使用多线程/多进程读取超大文本文件。这样可以分担读取超大文本文件的负担，从而防止内存吃不消。
使用单个较大的缓冲区读取超大文本文件。这样可以减少读取超大文本文件时的系统调用次数，从而提升读取速度，防止内存吃不消。]]></description>
</item>
<item>
    <title>Pandas条件定位单元格【类似select 字段 from 表 where 其他字段=某值】</title>
    <link>http://example.org/pandas%E6%9D%A1%E4%BB%B6%E5%AE%9A%E4%BD%8D%E5%8D%95%E5%85%83%E6%A0%BC%E7%B1%BB%E4%BC%BCsql%E6%9F%A5%E8%AF%A2/</link>
    <pubDate>Wed, 04 May 2022 21:02:56 &#43;0000</pubDate>
    <author>五彩斑斓的黑</author>
    <guid>http://example.org/pandas%E6%9D%A1%E4%BB%B6%E5%AE%9A%E4%BD%8D%E5%8D%95%E5%85%83%E6%A0%BC%E7%B1%BB%E4%BC%BCsql%E6%9F%A5%E8%AF%A2/</guid>
    <description><![CDATA[背景 实践当中，经常需要在Excel表中根据某一列的值去查看另一列对应的值。 比如，根据下表（假设存放在datas.xlsx）中姓名，查找其对应电话号码。 这里顺便记录一下这个网站可以方便的在excel、json、csv、yaml、markdown、xml、html表格之间互转：https://tableconvert.com/excel-to-markdown
序号 姓名 电话号码 年龄 性别 1 张三 4563453 39 男 2 李四 3453453 25 男 3 王五 2323423 18 女 4 李六 2342342 18 男 实操 1 2 3 import pandas as pd df = pd.read_excel(&#39;datas.xlsx&#39;,header=0,encoding=&#39;gbk&#39;) # header默认为0，即从第1行开始读取数据。 gbk为了支持中文 第一步 读取后Pandas默认会根据行数从0开始设置行索引，而不是将第一列作为行索引。 将姓名列设为行索引
1 df = df.set_index(&#39;姓名&#39;) 第二步 使用函数进行定位 at和loc两个函数均可，听说loc更快
1 df.at[&#39;李六&#39;,&#39;电话号码&#39;] 列数据转换 列数据转换，比如，将电话号码列数据转换为字符串类型
1 df[&#39; 电话号码&#39;] = df[&#39; 电话号码&#39;].apply(str) ]]></description>
</item>
<item>
    <title>Python pip 离线安装 package 方法总结（以 TensorFlow 为例）</title>
    <link>http://example.org/python-pip-%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85-package-%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93%E4%BB%A5-tensorflow-%E4%B8%BA%E4%BE%8B/</link>
    <pubDate>Wed, 02 Mar 2022 22:12:44 &#43;0000</pubDate>
    <author>五彩斑斓的黑</author>
    <guid>http://example.org/python-pip-%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85-package-%E6%96%B9%E6%B3%95%E6%80%BB%E7%BB%93%E4%BB%A5-tensorflow-%E4%B8%BA%E4%BE%8B/</guid>
    <description><![CDATA[文章背景 Python 环境安装 package，一般采用 pip 命令在线从 PyPI 是安装，这也是最方便的途径。但在某些情况下，要为一台离线的机器（比如在内网运行的一台生产服务器）安装 package，根本就连不上 PyPI。当然，大厂一般都有内网 PyPI 代理，只要改一下 --index-url 参数即可；但若代理也没有，只能手工将所有依赖到的 package 离线下载下来，放到离线的机器上安装。
下面是说明了整个探索过程。最终总结的方法，直接看文末的 “推荐方法” 即可。
心路历程 手工下载 之前我只是傻了吧唧，将要安装的 package（比如 tensorflow）从 PyPI 网站手工下载下来，放到内网机器直接安装，当然立马就会报 xxx 依赖不存在导致安装失败；根据报错提示再到 PyPI 上再下载这个 xxx 的 package 安装。而这个 xxx 可能又依赖另外一个 yyy。如此反复，整个过程有 30 个左右的多层依赖，搞了一下午，痛苦不堪。
读取 requirement 后来，我发现这些 package 里面都有 requirements 的声明文件，于是我就先解压出来看 requirement，这样至少直接依赖的 package 一下子都知道了。总比根据报错再一个一个下载快了。
但这还是不行，诉诸网络，发现了 pip download 可以解决这个问题，不过实际操作中还是有不少问题，下面以 TensorFlow 的离线安装为例说明。
pip download 总的思路是:
在 online 机器，通过 pip download tensorflow 命令（与 pip install 的区别就是，前者只下载不安装），将要安装的 xxx_package 以及所有依赖都下载到本地]]></description>
</item>
<item>
    <title>Python执行JS的几种方式</title>
    <link>http://example.org/python%E6%89%A7%E8%A1%8Cjs%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E5%BC%8F/</link>
    <pubDate>Wed, 16 Feb 2022 23:00:47 &#43;0000</pubDate>
    <author>五彩斑斓的黑</author>
    <guid>http://example.org/python%E6%89%A7%E8%A1%8Cjs%E7%9A%84%E5%87%A0%E7%A7%8D%E6%96%B9%E5%BC%8F/</guid>
    <description><![CDATA[https://juejin.cn/post/7015603661225066532
1.PyExecJS 经测试，比js2py快5倍多
安装依赖
pip3 install PyExecJS
使用方式
add.js 文件
1 2 3 4 function add(a,b){ return a+b; } 复制代码 py 文件去调用
1 2 3 4 5 6 7 8 import execjs with open(&#39;add.js&#39;, &#39;r&#39;, encoding=&#39;UTF-8&#39;) as f: js_code = f.read() context = execjs.compile(js_code) result = context.call(&#34;add&#34;, 2, 3) // 参数一为函数名，参数二和三为函数的参数 print(result) 复制代码 运行
2.js2py 安装依赖库
pip3 install js2py
还是上面的 add.js 文件
python 调用
1 2 3 4 5 6 7 8 import js2py with open(&#39;add.]]></description>
</item>
<item>
    <title>Python实现对大文件的增量读取</title>
    <link>http://example.org/python%E5%AE%9E%E7%8E%B0%E5%AF%B9%E5%A4%A7%E6%96%87%E4%BB%B6%E7%9A%84%E5%A2%9E%E9%87%8F%E8%AF%BB%E5%8F%96/</link>
    <pubDate>Wed, 01 Dec 2021 13:15:51 &#43;0000</pubDate>
    <author>五彩斑斓的黑</author>
    <guid>http://example.org/python%E5%AE%9E%E7%8E%B0%E5%AF%B9%E5%A4%A7%E6%96%87%E4%BB%B6%E7%9A%84%E5%A2%9E%E9%87%8F%E8%AF%BB%E5%8F%96/</guid>
    <description><![CDATA[背景 前段时间在做一个算法测试，需要对源于日志的数据进行分析才能获取到结果；日志文件较大，所以想要获取数据的变化曲线，增量读取是最好的方式。
网上有很多人的技术博客都是写的用for循环readline以及一个计数器去增量读取，假如文件很大，遍历一次太久。而且对于很多大文件的增量读取，如果遍历每一行比对历史记录的输出或者全都加载到内存通过历史记录的索引查找，是非常浪费资源的，
获取文件句柄的基本理论中就包含指针操作。linux的文件描述符的struct里有一个f_pos的这么个属性，里面存着文件当前读取位置，通过这个东东经过vfs的一系列映射就会得到硬盘存储的位置了，所以很直接，很快。
在Python中的读取文件的方法也有类似的属性。
具体实现 Python中相关方法的核心函数如下：
函数 作用 tell() 返回文件当前位置 seek() 从指定位置开始读取信息 其中seek()有三种模式：
f.seek(p,0) 移动当文件第p个字节处，绝对位置 f.seek(p,1) 移动到相对于当前位置之后的p个字节 f.seek(p,2) 移动到相对文章尾之后的p个字节 参考代码：
1 2 3 4 5 6 7 8 9 10 #!/usr/bin/python fd=open(&#34;test.txt&#34;,&#39;r&#39;) #获得一个句柄 for i in xrange(1,3): #读取三行数据 fd.readline() label=fd.tell() #记录读取到的位置 fd.close() #关闭文件 #再次阅读文件 fd=open(&#34;test.txt&#34;,&#39;r&#39;) #获得一个句柄 fd.seek(label,0)# 把文件读取指针移动到之前记录的位置 fd.readline() #接着上次的位置继续向下读取 拓展 如何得知这个大文件行数，以及变化 我的想法：
方式1： 遍历\n字符。
方式2： 开始时就在for循环中对fd.readline()计数，变化的部分（用上文说的seek、tell函数做）再用for循环fd.readline()进行统计。
如何避免文件读取时，内存溢出 可以通过 read 函数的chunk关键字来指定每次读区数据的大小 使用生成器确保只有在数据被调用时才会生成 具体方法封装如下：
1 2 3 4 5 6 7 8 9 def read_in_chunks(file_path, chunk=100 * 100): # 通过chunk指定每次读取文件的大小防止内存占用过大 file_object = open(file_path, &#34;r&#34;) while True: data = file_object.]]></description>
</item>
<item>
    <title>Python计算大文件行数方法及性能比较</title>
    <link>http://example.org/python%E8%AE%A1%E7%AE%97%E5%A4%A7%E6%96%87%E4%BB%B6%E8%A1%8C%E6%95%B0%E6%96%B9%E6%B3%95%E5%8F%8A%E6%80%A7%E8%83%BD%E6%AF%94%E8%BE%83/</link>
    <pubDate>Wed, 01 Dec 2021 13:01:57 &#43;0000</pubDate>
    <author>五彩斑斓的黑</author>
    <guid>http://example.org/python%E8%AE%A1%E7%AE%97%E5%A4%A7%E6%96%87%E4%BB%B6%E8%A1%8C%E6%95%B0%E6%96%B9%E6%B3%95%E5%8F%8A%E6%80%A7%E8%83%BD%E6%AF%94%E8%BE%83/</guid>
    <description><![CDATA[如何使用Python快速高效地统计出大文件的总行数, 下面是一些实现方法和性能的比较。
实现方法 readlines方法读所有行 1 2 def readline_count(file_name): return len(open(file_name).readlines()) 依次读取每行 1 2 3 4 5 def simple_count(file_name): lines = 0 for _ in open(file_name): lines += 1 return lines 使用sum函数计数 1 2 def sum_count(file_name): return sum(1 for _ in open(file_name)) enumerate枚举计数 1 2 3 4 5 def enumerate_count(file_name): with open(file_name) as f: for count, _ in enumerate(f, 1): pass return count buff+count每次读取固定大小,然后统计行数 1 2 3 4 5 6 7 8 9 def buff_count(file_name): with open(file_name, &#39;rb&#39;) as f: count = 0 buf_size = 1024 * 1024 buf = f.]]></description>
</item>
<item>
    <title>pip离线安装依赖库</title>
    <link>http://example.org/pip%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85%E4%BE%9D%E8%B5%96%E5%BA%93/</link>
    <pubDate>Wed, 01 Dec 2021 10:43:09 &#43;0000</pubDate>
    <author>五彩斑斓的黑</author>
    <guid>http://example.org/pip%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85%E4%BE%9D%E8%B5%96%E5%BA%93/</guid>
    <description><![CDATA[记录下命令，下载所需依赖库，在不能联网的机器上离线安装。
1 2 3 4 5 6 7 8 9 10 11 # 查看 pip list # 依赖库信息格式输出 pip freeze &gt; requirements.txt # 仅下载 pip download -r requirements.txt # 安装 pip install --no-index --find-links=dir_path -r requirements.txt ]]></description>
</item>
<item>
    <title>aiohttp使用回调函数边请求边处理</title>
    <link>http://example.org/aiohttp%E4%BD%BF%E7%94%A8%E5%9B%9E%E8%B0%83%E5%87%BD%E6%95%B0%E8%BE%B9%E8%AF%B7%E6%B1%82%E8%BE%B9%E5%A4%84%E7%90%86/</link>
    <pubDate>Sat, 27 Nov 2021 16:58:17 &#43;0000</pubDate>
    <author>五彩斑斓的黑</author>
    <guid>http://example.org/aiohttp%E4%BD%BF%E7%94%A8%E5%9B%9E%E8%B0%83%E5%87%BD%E6%95%B0%E8%BE%B9%E8%AF%B7%E6%B1%82%E8%BE%B9%E5%A4%84%E7%90%86/</guid>
    <description><![CDATA[我们平时使用Requests的时候，一般是这样写代码的：
1 2 3 4 5 6 7 import requests def parse(html): print(&#39;对 html 进行处理&#39;) html = requests.get(&#39;url&#39;) parse(html) 这是一种非常常见的直线性思维，我先请求网站拿到 html，然后我再把 html 传给负责处理的函数。在整个过程中，“我“担任着调度的角色。
在这种思维方式的影响下，有些同学即使在使用aiohttp写异步爬虫，也是这样写的：
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 import aiohttp import asyncio async def request(url): async with aiohttp.ClientSession() as session: resp = await session.get(url) html = await resp.text(encoding=&#39;utf-8&#39;) def parse(html): print(&#39;处理 html&#39;) async def main(): url_list = [url1, url2, url3, url4] tasks = [] for url in url_list: tasks.]]></description>
</item>
</channel>
</rss>
